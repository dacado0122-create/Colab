{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMc5q0hfbq9ucKigxhkRtvU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dacado0122-create/Colab/blob/main/codigos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**codigo 1.1**"
      ],
      "metadata": {
        "id": "PWMzpWS0hn2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Parámetros del problema\n",
        "D = 1e-3\n",
        "r = 1.0\n",
        "N = 101        # puntos de malla en x,y\n",
        "T = 1.0        # tiempo final\n",
        "\n",
        "# Malla espacial\n",
        "x = np.linspace(0, 1, N)\n",
        "y = np.linspace(0, 1, N)\n",
        "dx = x[1] - x[0]\n",
        "\n",
        "# Condición CFL\n",
        "dt_max = dx**2 / (4 * D)\n",
        "dt = 1e-3  # seguro y cumple CFL\n",
        "assert dt < dt_max, \"dt NO cumple CFL\"\n",
        "\n",
        "Nt = int(T / dt)\n",
        "\n",
        "# Inicializar solución\n",
        "X, Y = np.meshgrid(x, y, indexing='ij')\n",
        "u = np.exp(-((X - 0.5)**2 + (Y - 0.5)**2) / 0.05)\n",
        "\n",
        "# Tensor de salida\n",
        "Ground_Truth = np.zeros((Nt, N, N))\n",
        "Ground_Truth[0] = u.copy()\n",
        "\n",
        "def laplaciano_periodico(u):\n",
        "    \"\"\"Implementa el stencil de 5 puntos con fronteras periódicas.\"\"\"\n",
        "    return (\n",
        "        np.roll(u, 1, axis=0) + np.roll(u, -1, axis=0) +\n",
        "        np.roll(u, 1, axis=1) + np.roll(u, -1, axis=1) -\n",
        "        4 * u\n",
        "    ) / dx**2\n",
        "\n",
        "# Evolución temporal\n",
        "for n in range(1, Nt):\n",
        "    lap = laplaciano_periodico(u)\n",
        "    reaction = r * u * (1 - u)\n",
        "    u = u + dt * (D * lap + reaction)\n",
        "    Ground_Truth[n] = u.copy()\n",
        "\n",
        "print(\"Ground_Truth shape:\", Ground_Truth.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSGbLWOyh2Es",
        "outputId": "6fefe7ca-0046-47da-8bb4-670c10df4b67"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground_Truth shape: (1000, 101, 101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**codigo 1.2**"
      ],
      "metadata": {
        "id": "_bb0RFdtijCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Cargar datos Ground Truth\n",
        "# -----------------------------\n",
        "GT = Ground_Truth  # (Nt, Nx, Ny)\n",
        "Nt, Nx, Ny = GT.shape\n",
        "\n",
        "# Crear malla de coordenadas\n",
        "x = np.linspace(0, 1, Nx)\n",
        "y = np.linspace(0, 1, Ny)\n",
        "t = np.linspace(0, 1, Nt)\n",
        "\n",
        "X, Y, T = np.meshgrid(x, y, t, indexing='ij')\n",
        "\n",
        "coords = np.stack(\n",
        "    [X.flatten(), Y.flatten(), T.flatten()],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "values = GT.transpose(1, 2, 0).flatten()  # u(x,y,t)\n",
        "\n",
        "# Pasar a tensores PyTorch\n",
        "coords = torch.tensor(coords, dtype=torch.float32)\n",
        "values = torch.tensor(values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Definir red neuronal\n",
        "# -----------------------------\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(3, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = DNN()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Entrenamiento\n",
        "# -----------------------------\n",
        "batch_size = 1024\n",
        "n_epochs = 2000\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(coords, values)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for xb, yb in dataloader:\n",
        "        pred = model(xb)\n",
        "        loss = loss_fn(pred, yb)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch} - Loss: {loss.item():.6f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Evaluación / extrapolación\n",
        "# -----------------------------\n",
        "# Punto fuera del tiempo de entrenamiento (t > T)\n",
        "test_point = torch.tensor([[0.2, 0.8, 1.2]], dtype=torch.float32)\n",
        "print(\"Extrapolación:\", model(test_point).item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8tvudEuirdD",
        "outputId": "18c57e67-45bf-45c9-8916-dc3d854783e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss: 0.000033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**codigo 2.1**"
      ],
      "metadata": {
        "id": "x_rW78_FkO4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Definición del PINN\n",
        "# -----------------------------\n",
        "class PINN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(3, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = PINN()\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Derivación automática\n",
        "# -----------------------------\n",
        "def gradients(u, x):\n",
        "    \"\"\"Calcula du/dx para un escalar u y vector x=[x,y,t].\"\"\"\n",
        "    return torch.autograd.grad(\n",
        "        u, x,\n",
        "        grad_outputs=torch.ones_like(u),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0]\n",
        "\n",
        "def laplacian(u, x):\n",
        "    \"\"\"Devuelve u_xx + u_yy mediante autograd.\"\"\"\n",
        "    grads = gradients(u, x)\n",
        "    ux, uy, ut = grads[:, 0], grads[:, 1], grads[:, 2]\n",
        "\n",
        "    ux_x = torch.autograd.grad(\n",
        "        ux, x,\n",
        "        grad_outputs=torch.ones_like(ux),\n",
        "        create_graph=True\n",
        "    )[0][:, 0]\n",
        "\n",
        "    uy_y = torch.autograd.grad(\n",
        "        uy, x,\n",
        "        grad_outputs=torch.ones_like(uy),\n",
        "        create_graph=True\n",
        "    )[0][:, 1]\n",
        "\n",
        "    return ux_x + uy_y, ut\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Pérdida del residuo\n",
        "# -----------------------------\n",
        "D = 1e-3\n",
        "r = 1.0\n",
        "\n",
        "def residual_loss(model, Xf):\n",
        "    Xf.requires_grad_(True)\n",
        "    u = model(Xf)\n",
        "    lap_u, u_t = laplacian(u, Xf)\n",
        "    res = u_t - D * lap_u - r * u * (1 - u)\n",
        "    return torch.mean(res**2)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Pérdida de condición inicial\n",
        "# -----------------------------\n",
        "def ic_loss(model, X0, U0):\n",
        "    u_pred = model(X0)\n",
        "    return torch.mean((u_pred - U0)**2)\n",
        "\n",
        "==\n",
        "\n",
        "# Puntos de colocación en el dominio\n",
        "Nf = 5000\n",
        "Xf = torch.rand(Nf, 3)\n",
        "\n",
        "# Puntos para condición inicial (t = 0)\n",
        "N0 = 1000\n",
        "X0 = torch.rand(N0, 3)\n",
        "X0[:, 2] = 0.0\n",
        "\n",
        "# Condición inicial\n",
        "U0 = torch.exp(\n",
        "    -((X0[:, 0] - 0.5)**2 + (X0[:, 1] - 0.5)**2) / 0.05\n",
        ").unsqueeze(1)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Entrenamiento\n",
        "# -----------------------------\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(5000):\n",
        "    loss_res = residual_loss(model, Xf)\n",
        "    loss_ic = ic_loss(model, X0, U0)\n",
        "    loss = loss_res + loss_ic\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} | Loss: {loss.item():.6f}\")\n"
      ],
      "metadata": {
        "id": "YVNr51XdkPK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**codigo 3**"
      ],
      "metadata": {
        "id": "iAGB0zX9kY9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Definición del bloque PDE-Net\n",
        "# -----------------------------\n",
        "class PDENet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=1, out_channels=16,\n",
        "            kernel_size=3, padding=1, padding_mode='circular'\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=16, out_channels=1,\n",
        "            kernel_size=3, padding=1, padding_mode='circular'\n",
        "        )\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "        # Inicialización física del primer kernel\n",
        "        laplace = torch.tensor([\n",
        "            [0., 1., 0.],\n",
        "            [1., -4., 1.],\n",
        "            [0., 1., 0.]\n",
        "        ])\n",
        "\n",
        "        # Ajustar dimensiones: out_channels x in_channels x 3 x 3\n",
        "        self.conv1.weight.data[0, 0, :, :] = 0.01 * laplace\n",
        "\n",
        "    def forward(self, U):\n",
        "        x = self.activation(self.conv1(U))\n",
        "        x = self.conv2(x)\n",
        "        return U + x  # bloque residual\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Preparación de datos\n",
        "# -----------------------------\n",
        "# Ground_Truth: (Nt, Nx, Ny)\n",
        "GT_tensor = torch.tensor(Ground_Truth, dtype=torch.float32)\n",
        "GT_tensor = GT_tensor.unsqueeze(1)  # -> (Nt, 1, Nx, Ny)\n",
        "\n",
        "U_t = GT_tensor[:-1]    # (Nt-1, 1, Nx, Ny)\n",
        "U_tp1 = GT_tensor[1:]  # (Nt-1, 1, Nx, Ny)\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(U_t, U_tp1)\n",
        "loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Entrenamiento\n",
        "# -----------------------------\n",
        "model = PDENet()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "for epoch in range(2000):\n",
        "    for u, target in loader:\n",
        "        pred = model(u)\n",
        "        loss = loss_fn(pred, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoch {epoch} - Loss: {loss.item():.6e}\")\n"
      ],
      "metadata": {
        "id": "3Dyo-Cghkwxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**codigo 4**"
      ],
      "metadata": {
        "id": "vQTE-h0OlIvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, patch_size, emb_dim):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Linear(patch_size * patch_size, emb_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, 1, N, N)\n",
        "        B, _, N, _ = x.shape\n",
        "        P = self.patch_size\n",
        "        x = x.unfold(2, P, P).unfold(3, P, P)\n",
        "        x = x.contiguous().view(B, -1, P * P)\n",
        "        return self.proj(x)\n",
        "\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, patch_size=8, emb_dim=128, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.embed = PatchEmbedding(patch_size, emb_dim)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=emb_dim,\n",
        "            nhead=n_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=4\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(emb_dim, patch_size * patch_size)\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def forward(self, U):\n",
        "        Z = self.embed(U)\n",
        "        Z = self.transformer(Z)\n",
        "        Z = self.fc(Z)\n",
        "\n",
        "        # reconstrucción\n",
        "        B, NP, _ = Z.shape\n",
        "        P = self.patch_size\n",
        "        N = int((NP) ** 0.5)\n",
        "\n",
        "        Z = Z.view(B, N, N, P, P)\n",
        "        Z = Z.permute(0, 1, 3, 2, 4).contiguous()\n",
        "\n",
        "        return Z.view(B, 1, N * P, N * P)\n"
      ],
      "metadata": {
        "id": "RMR6lBE4lM_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**codigo 5.1**"
      ],
      "metadata": {
        "id": "zY2ADPQCltCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.conv = nn.Conv2d(\n",
        "            input_dim + hidden_dim,\n",
        "            4 * hidden_dim,\n",
        "            kernel_size,\n",
        "            padding=kernel_size // 2\n",
        "        )\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        h_cur, c_cur = hidden\n",
        "\n",
        "        combined = torch.cat([x, h_cur], dim=1)\n",
        "        gates = self.conv(combined)\n",
        "\n",
        "        i, f, o, g = torch.split(gates, self.hidden_dim, dim=1)\n",
        "\n",
        "        i = torch.sigmoid(i)\n",
        "        f = torch.sigmoid(f)\n",
        "        o = torch.sigmoid(o)\n",
        "        g = torch.tanh(g)\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "\n",
        "class ConvLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            ConvLSTMCell(\n",
        "                input_dim if i == 0 else hidden_dim,\n",
        "                hidden_dim,\n",
        "                kernel_size\n",
        "            )\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x_seq):\n",
        "        batch_size, seq_len, _, height, width = x_seq.shape\n",
        "\n",
        "        h = [\n",
        "            torch.zeros(batch_size, self.hidden_dim, height, width).to(x_seq.device)\n",
        "            for _ in range(self.num_layers)\n",
        "        ]\n",
        "        c = [\n",
        "            torch.zeros(batch_size, self.hidden_dim, height, width).to(x_seq.device)\n",
        "            for _ in range(self.num_layers)\n",
        "        ]\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x = x_seq[:, t]\n",
        "\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                h[i], c[i] = layer(x, (h[i], c[i]))\n",
        "                x = h[i]\n",
        "\n",
        "            outputs.append(h[-1])\n",
        "\n",
        "        return torch.stack(outputs, dim=1)\n"
      ],
      "metadata": {
        "id": "6na4QAr6lvz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**codigo 5.2**"
      ],
      "metadata": {
        "id": "WiNzFCsjmLW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def variational_loss(model, X, T, k=1):\n",
        "    X.requires_grad_(True)\n",
        "\n",
        "    u = model(X)\n",
        "\n",
        "    grads = torch.autograd.grad(\n",
        "        u, X,\n",
        "        grad_outputs=torch.ones_like(u),\n",
        "        create_graph=True\n",
        "    )[0]\n",
        "\n",
        "    ux, uy, ut = grads[:, 0], grads[:, 1], grads[:, 2]\n",
        "    x, y = X[:, 0], X[:, 1]\n",
        "\n",
        "    v = torch.sin(k * torch.pi * x) * torch.sin(k * torch.pi * y)\n",
        "    vx = k * torch.pi * torch.cos(k * torch.pi * x) * torch.sin(k * torch.pi * y)\n",
        "    vy = k * torch.pi * torch.sin(k * torch.pi * x) * torch.cos(k * torch.pi * y)\n",
        "\n",
        "    integrand = ut * v + D * (ux * vx + uy * vy) - r * u * (1 - u) * v\n",
        "\n",
        "    volume = T\n",
        "    integral = volume * torch.mean(integrand)\n",
        "\n",
        "    return integral ** 2\n"
      ],
      "metadata": {
        "id": "1JuUrkBDmTBk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}